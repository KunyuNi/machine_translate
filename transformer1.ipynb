{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "sys.version_info(major=3, minor=6, micro=13, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.4\n",
      "numpy 1.19.2\n",
      "pandas 1.1.5\n",
      "tensorflow 2.2.0\n",
      "tensorflow.keras 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd , tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  1 10:52:20 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.84       Driver Version: 460.84       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:65:00.0 Off |                  N/A |\r\n",
      "|  0%   27C    P8    29W / 350W |   1932MiB / 24267MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A    135103      C   ...conda3/envs/ml/bin/python      965MiB |\r\n",
      "|    0   N/A  N/A    135402      C   ...conda3/envs/ml/bin/python      965MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. loads data\n",
    "# 2. preprocesses data -> dataset\n",
    "# 3. tools\n",
    "# 3.1 generates position embedding\n",
    "# 3.2 create mask. (a. padding, b. decoder)\n",
    "# 3.3 scaled_dot_product_attention\n",
    "# 4. builds model 分为以下6步\n",
    "    # 4.1 MultiheadAttention\n",
    "    # 4.2 EncoderLayer\n",
    "    # 4.3 DecoderLayer\n",
    "    # 4.4 EncoderModel\n",
    "    # 4.5 DecoderModel\n",
    "    # 4.6 Transformer\n",
    "# 5. optimizer & loss\n",
    "# 6. train step -> train\n",
    "# 7. Evaluate and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='ted_hrlr_translate',\n",
      "    version=0.0.1,\n",
      "    description='Data sets derived from TED talk transcripts for comparing similar language pairs\n",
      "where one is high resource and the other is low resource.\n",
      "',\n",
      "    urls=['https://github.com/neulab/word-embeddings-for-nmt'],\n",
      "    features=Translation({\n",
      "        'en': Text(shape=(), dtype=tf.string),\n",
      "        'pt': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=54781,\n",
      "    splits={\n",
      "        'test': 1803,\n",
      "        'train': 51785,\n",
      "        'validation': 1193,\n",
      "    },\n",
      "    supervised_keys=('pt', 'en'),\n",
      "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
      "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
      "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
      "      booktitle = {HLT-NAACL},\n",
      "      year    = {2018},\n",
      "      }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "#葡萄牙语到英语，这个是基于subword的\n",
    "examples, info = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                           with_info = True,\n",
    "                           as_supervised = True)\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "print(info)#info里是数据集的描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'aterrei pela primeira vez em roma , it\\xc3\\xa1lia .'\n",
      "b'i landed for the first time in rome , italy .'\n",
      "\n",
      "b'os astr\\xc3\\xb3nomos acreditam que cada estrela da gal\\xc3\\xa1xia tem um planeta , e especulam que at\\xc3\\xa9 um quinto deles tem um planeta do tipo da terra que poder\\xc3\\xa1 ter vida , mas ainda n\\xc3\\xa3o vimos nenhum deles .'\n",
      "b\"astronomers now believe that every star in the galaxy has a planet , and they speculate that up to one fifth of them have an earth-like planet that might be able to harbor life , but we have n't seen any of them .\"\n",
      "\n",
      "b'houve alega\\xc3\\xa7\\xc3\\xb5es de intimida\\xc3\\xa7\\xc3\\xa3o nas mesas de voto , de urnas de voto serem roubadas .'\n",
      "b'there were claims of intimidation at the polling stations , of ballot boxes being stolen .'\n",
      "\n",
      "b'lembrem-se do que resgatou os desfavorecidos .'\n",
      "b'remember what pulled up the poor .'\n",
      "\n",
      "b'o problema \\xc3\\xa9 que nunca vivi l\\xc3\\xa1 um \\xc3\\xbanico dia .'\n",
      "b\"except , i 've never lived one day of my life there .\"\n",
      "\n",
      "<_OptionsDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "#葡萄牙语中有一些特除的字符，用转义字符来打印\n",
    "for pt, en in train_examples.take(5):\n",
    "    print(pt.numpy())\n",
    "    print(en.numpy())\n",
    "    print()\n",
    "print(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里运行要点时间\n",
    "#我们自己转为subword数据集，2**13是8192，build_from_corpus\n",
    "en_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples),\n",
    "    target_vocab_size = 2 ** 13)\n",
    "pt_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples),\n",
    "    target_vocab_size = 2 ** 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string is Transformer is awesome.\n",
      "7915 --> \"T\"-->1\n",
      "1248 --> \"ran\"-->3\n",
      "7946 --> \"s\"-->1\n",
      "7194 --> \"former \"-->7\n",
      "13 --> \"is \"-->3\n",
      "2799 --> \"awesome\"-->7\n",
      "7877 --> \".\"-->1\n"
     ]
    }
   ],
   "source": [
    "#测试一个字符串,subword里边是包含空格的\n",
    "sample_string = \"Transformer is awesome.\"\n",
    "\n",
    "tokenized_string = en_tokenizer.encode(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "origin_string = en_tokenizer.decode(tokenized_string)\n",
    "print('The original string is {}'.format(origin_string))\n",
    "\n",
    "assert origin_string == sample_string\n",
    "\n",
    "for token in tokenized_string:\n",
    "    print('{} --> \"{}\"-->{}'.format(token, en_tokenizer.decode([token]),len(en_tokenizer.decode([token]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8214"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8087"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 20000\n",
    "batch_size = 64\n",
    "max_length = 40  #输入和输出的最大长度是40\n",
    "\n",
    "#把两段文本转为subword形式，\n",
    "def encode_to_subword(pt_sentence, en_sentence):\n",
    "    pt_sequence = [pt_tokenizer.vocab_size] \\\n",
    "    + pt_tokenizer.encode(pt_sentence.numpy()) \\\n",
    "    + [pt_tokenizer.vocab_size + 1]\n",
    "    en_sequence = [en_tokenizer.vocab_size] \\\n",
    "    + en_tokenizer.encode(en_sentence.numpy()) \\\n",
    "    + [en_tokenizer.vocab_size + 1]\n",
    "    return pt_sequence, en_sequence\n",
    "\n",
    "#用tf的API消去大于最大长度的\n",
    "def filter_by_max_length(pt, en):\n",
    "    return tf.logical_and(tf.size(pt) <= max_length,\n",
    "                          tf.size(en) <= max_length)\n",
    "#用py_function封装一下encode_to_subword\n",
    "def tf_encode_to_subword(pt_sentence, en_sentence):\n",
    "    return tf.py_function(encode_to_subword,\n",
    "                          [pt_sentence, en_sentence],\n",
    "                          [tf.int64, tf.int64])\n",
    "#把所有句子变为subword，subword都变为id\n",
    "train_dataset = train_examples.map(tf_encode_to_subword)\n",
    "train_dataset = train_dataset.filter(filter_by_max_length)\n",
    "#接着做洗牌，padding，batch -1，-1代表两个维度，每个维度都在当前维度下扩展到最高的值\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size).padded_batch(\n",
    "    batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "valid_dataset = val_examples.map(tf_encode_to_subword)\n",
    "valid_dataset = valid_dataset.filter(\n",
    "    filter_by_max_length).padded_batch(\n",
    "    batch_size, padded_shapes=([-1], [-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40) (64, 39)\n",
      "(64, 39) (64, 37)\n",
      "(64, 40) (64, 38)\n",
      "(64, 39) (64, 36)\n",
      "(64, 40) (64, 39)\n"
     ]
    }
   ],
   "source": [
    "for pt_batch, en_batch in train_dataset.take(5):\n",
    "    print(pt_batch.shape, en_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 512)\n",
      "(50, 256)\n",
      "(50, 256)\n",
      "tf.Tensor(\n",
      "[[[ 0.          0.          0.         ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.84147096  0.8218562   0.8019618  ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.9092974   0.9364147   0.95814437 ...  1.          1.\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.12357312  0.97718984 -0.24295525 ...  0.9999863   0.99998724\n",
      "    0.99998814]\n",
      "  [-0.76825464  0.7312359   0.63279754 ...  0.9999857   0.9999867\n",
      "    0.9999876 ]\n",
      "  [-0.95375264 -0.14402692  0.99899054 ...  0.9999851   0.9999861\n",
      "    0.9999871 ]]], shape=(1, 50, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#第三步，写一些工具函数\n",
    "# PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "# PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "#pos 和i都是矩阵\n",
    "# pos.shape: [sentence_length, 1]\n",
    "# i.shape  : [1, d_model]\n",
    "# result.shape: [sentence_length, d_model]\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000,\n",
    "                               (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "#计算位置信息\n",
    "def get_position_embedding(sentence_length, d_model):\n",
    "    #sentence_length和d_model都扩展为矩阵\n",
    "#     print(np.arange(sentence_length)[:, np.newaxis])\n",
    "#     print(np.arange(d_model)[np.newaxis, :])\n",
    "    #pos是0到49，就是词的位置，i是从0到511，总计512，和dim相等\n",
    "    angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    print(angle_rads.shape)\n",
    "    # sines.shape: [sentence_length, d_model / 2]\n",
    "    # cosines.shape: [sentence_length, d_model / 2]\n",
    "    print(angle_rads[:, 0::2].shape)\n",
    "    print(angle_rads[:, 1::2].shape)\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    #把sines和cosines进行拼接\n",
    "    # position_embedding.shape: [sentence_length, d_model]\n",
    "    position_embedding = np.concatenate([sines, cosines], axis = -1)\n",
    "    #进行维度扩展\n",
    "    # position_embedding.shape: [1, sentence_length, d_model]\n",
    "    position_embedding = position_embedding[np.newaxis, ...]\n",
    "    #变为float32类型\n",
    "    return tf.cast(position_embedding, dtype=tf.float32)\n",
    "\n",
    "position_embedding = get_position_embedding(50, 512)\n",
    "print(position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#这个图和我们原理中展示的横纵坐标是颠倒的\n",
    "def plot_position_embedding(position_embedding):\n",
    "    plt.pcolormesh(position_embedding[0], cmap = 'RdBu')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "plot_position_embedding(position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如何生成mask\n",
    "# 1. padding mask, 2. look ahead\n",
    "\n",
    "# batch_data.shape: [batch_size, seq_len]\n",
    "def create_padding_mask(batch_data):\n",
    "    padding_mask = tf.cast(tf.math.equal(batch_data, 0), tf.float32)\n",
    "    # [batch_size, 1, 1, seq_len]\n",
    "    return padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "#设置3x5矩阵，0都是padding，是零的得到的都是1，其他的都是零\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_weights.shape: [3,3]\n",
    "#第一个位置代表第一个单词和自己的attention，第二位置是第二个单词和第一个单词的attention\n",
    "#看不到后面的词刚好是下三角，使用库函数来实现\n",
    "# [[1, 0, 0],\n",
    "#  [4, 5, 0],\n",
    "#  [7, 8, 9]]\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask # (seq_len, seq_len)\n",
    "\n",
    "#前面看不到后面的padding，矩阵下面全部为0\n",
    "# 在mask里，应该被忽略的我们会设成1，应该被保留的会设成0，\n",
    "# 而如果mask相应位置上为1，那么我们就给对应的logits \n",
    "# 加上一个超级小的负数， -1000000000， 这样，\n",
    "# 对应的logits也就变成了一个超级小的数。然后在计算softmax的时候，\n",
    "# 一个超级小的数的指数会无限接近与0。也就是它对应的attention的权重就是0了,\n",
    "# 下面可以看到\n",
    "create_look_ahead_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考原理文档，q是query，k，v代表k和value，q和k做完矩阵乘法后，做mask\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - q: shape == (..., seq_len_q, depth)\n",
    "    - k: shape == (..., seq_len_k, depth)\n",
    "    - v: shape == (..., seq_len_v, depth_v)\n",
    "    - seq_len_k == seq_len_v  这两个是相等的\n",
    "    - mask: shape == (..., seq_len_q, seq_len_k)\n",
    "    Returns:\n",
    "    - output: weighted sum\n",
    "    - attention_weights: weights of attention\n",
    "    \"\"\"\n",
    "    #计算attentions时，我们只用了后两维在计算\n",
    "    # transpose_b代表第二个矩阵是否做转置\n",
    "    # matmul_qk.shape: (..., seq_len_q, seq_len_k)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
    "    \n",
    "    #获得dk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    #然后根据文档中的公式除以dk\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    #如果mask不是空的话，给scaled_attention_logits加一个mask（缩放）\n",
    "    if mask is not None:\n",
    "        # 使得在softmax后值趋近于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # attention_weights.shape: (..., seq_len_q, seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(\n",
    "        scaled_attention_logits, axis = -1)\n",
    "    \n",
    "    #根据原理图，v和attention_weights进行矩阵乘法\n",
    "    # output.shape: (..., seq_len_q, depth_v)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "#调用上面的函数，去验证\n",
    "def print_scaled_dot_product_attention(q, k, v):\n",
    "    temp_out, temp_att = scaled_dot_product_attention(q, k, v, None)\n",
    "    print(\"Attention weights are:\")\n",
    "    print(temp_att)\n",
    "    print(\"Output is:\")\n",
    "    print(temp_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#我们定义一个测试的Q，K，V\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32) # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32) # (4, 2)\n",
    "\n",
    "temp_q1 = tf.constant([[0, 10, 0]], dtype=tf.float32) # (1, 3)\n",
    "#可以把这句注释，它的作用是做四舍五入，让结果清爽\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "print_scaled_dot_product_attention(temp_q1, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q2 = tf.constant([[0, 0, 10]], dtype=tf.float32) # (1, 3)\n",
    "#0.  0.  0.5 0.5 会和temp_v去做平均，因此得到的是550,5.5\n",
    "print_scaled_dot_product_attention(temp_q2, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q3 = tf.constant([[10, 10, 0]], dtype=tf.float32) # (1, 3)\n",
    "print_scaled_dot_product_attention(temp_q3, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[ 10.    0. ]\n",
      " [550.    5.5]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#拼起来再来测试\n",
    "temp_q4 = tf.constant([[0, 10, 0],\n",
    "                       [0, 0, 10],\n",
    "                       [10, 10, 0]], dtype=tf.float32) # (3, 3)\n",
    "print_scaled_dot_product_attention(temp_q4, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 256)\n",
      "--------------------------------------------------\n",
      "(1, 60, 512)\n",
      "(1, 60, 512)\n",
      "(1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "#多头注意力的实现\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    理论上:\n",
    "    x -> Wq0 -> q0\n",
    "    x -> Wk0 -> k0\n",
    "    x -> Wv0 -> v0\n",
    "    \n",
    "    实战中:把三个概念区分开\n",
    "    q -> Wq0 -> q0\n",
    "    k -> Wk0 -> k0\n",
    "    v -> Wv0 -> v0\n",
    "    \n",
    "    实战中技巧：q乘以W得到一个大的Q，然后分割为多个小q\n",
    "    q -> Wq -> Q -> split -> q0, q1, q2...\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        \n",
    "        #这里对应的大Q变小q怎么变，层次\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.WQ = keras.layers.Dense(self.d_model)\n",
    "        self.WK = keras.layers.Dense(self.d_model)\n",
    "        self.WV = keras.layers.Dense(self.d_model)\n",
    "        #这里是拼接\n",
    "        self.dense = keras.layers.Dense(self.d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # x.shape: (batch_size, seq_len, d_model)\n",
    "        # d_model = num_heads * depth\n",
    "        #把x变为下面维度，用reshape\n",
    "        # x -> (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        x = tf.reshape(x,\n",
    "                       (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])#轴滚动\n",
    "    \n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        #经过Q K V变化\n",
    "        print(q.shape)\n",
    "        q = self.WQ(q) # q.shape: (batch_size, seq_len_q, d_model)\n",
    "        k = self.WK(k) # k.shape: (batch_size, seq_len_k, d_model)\n",
    "        v = self.WV(v) # v.shape: (batch_size, seq_len_v, d_model)\n",
    "        print('-'*50)\n",
    "        print(q.shape)\n",
    "        # q.shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        # k.shape: (batch_size, num_heads, seq_len_k, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        # v.shape: (batch_size, num_heads, seq_len_v, depth)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        #开始做缩放点积，得到的多头的信息存在在num_heads，depth上\n",
    "        # scaled_attention_outputs.shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention_outputs, attention_weights = \\\n",
    "        scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        #因此这里做一下转置，让num_heads，depth在后面\n",
    "        # scaled_attention_outputs.shape: (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention_outputs = tf.transpose(\n",
    "            scaled_attention_outputs, perm = [0, 2, 1, 3])\n",
    "        \n",
    "        #对注意力进行合并\n",
    "        # concat_attention.shape: (batch_size, seq_len_q, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention_outputs,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        #多头注意力计算完毕\n",
    "        # output.shape : (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "#创建一份虚拟数据\n",
    "y = tf.random.uniform((1, 60, 256)) # (batch_size, seq_len_q, dim)\n",
    "#开始计算，把y既当q，又当k，v\n",
    "output, attn = temp_mha(y, y, y, mask = None)\n",
    "print(output.shape)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义我们的feed_forward_network，d_model节点数\n",
    "def feed_forward_network(d_model, dff):\n",
    "    # dff: dim of feed forward network.\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Dense(dff, activation='relu'),\n",
    "        keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "sample_ffn = feed_forward_network(512, 2048)\n",
    "#给一个输入测试\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n",
      "--------------------------------------------------\n",
      "(64, 50, 512)\n",
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "#自定义EncoderLayer\n",
    "class EncoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x -> self attention -> add & normalize & dropout\n",
    "      -> feed_forward -> add & normalize & dropout\n",
    "    原理对应文档Add & Normalize 标题下的图\n",
    "    \"\"\"\n",
    "    #d_model 给self attention和feed_forward_network，num_heads给self_attention用的\n",
    "    #dff给feed_forward_network，rate是做dropout的\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        # epsilon 将小浮点数添加到方差以避免被零除\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        #下面两个层次用了做dropout，每次有10%的几率被drop掉\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, encoder_padding_mask):\n",
    "        # x.shape          : (batch_size, seq_len, dim=d_model)\n",
    "        # attn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out1.shape       : (batch_size, seq_len, d_model)\n",
    "        #x作为q，k，v  原理对应文档Add & Normalize 标题下的图\n",
    "        attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        #dim=d_model 两个必须相等，这样x才可以和attn_output做加法\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out2.shape      : (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "#来测试，结果和我们最初的输入维度一致，相当于做了两次残差连接\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_input = tf.random.uniform((64, 50, 512))\n",
    "sample_output = sample_encoder_layer(sample_input, False, None)\n",
    "print(sample_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.06375826, -0.06400497,  0.05346561, ..., -0.05691981,\n",
       "         -0.04647036,  0.00539462],\n",
       "        [-0.05929273, -0.04325018,  0.06238494, ..., -0.01568025,\n",
       "         -0.07140822, -0.00790648],\n",
       "        [-0.00829075, -0.0028923 , -0.00834498, ..., -0.02490101,\n",
       "          0.05247945, -0.01541715],\n",
       "        ...,\n",
       "        [-0.00755053, -0.03498329,  0.02771617, ..., -0.05648112,\n",
       "         -0.06326326,  0.00368494],\n",
       "        [ 0.02549014,  0.05162101, -0.05393816, ...,  0.01787171,\n",
       "         -0.02584456, -0.05261625],\n",
       "        [ 0.03859899,  0.0114012 , -0.01101557, ..., -0.02293916,\n",
       "          0.04842018,  0.07185272]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[ 0.0125117 , -0.01340254, -0.02917468, ...,  0.04796708,\n",
       "         -0.05842356,  0.02231302],\n",
       "        [-0.00604951, -0.03350854,  0.04712461, ...,  0.06206279,\n",
       "         -0.06106724,  0.03713901],\n",
       "        [-0.03803898,  0.07468631,  0.071755  , ...,  0.00588468,\n",
       "          0.06110489, -0.02694546],\n",
       "        ...,\n",
       "        [ 0.01886743, -0.02987492,  0.03966364, ..., -0.02063729,\n",
       "          0.01666262, -0.03778541],\n",
       "        [ 0.05934999,  0.02100583,  0.06590081, ...,  0.05117769,\n",
       "          0.04949898, -0.06356725],\n",
       "        [-0.06085218, -0.05688921,  0.06007395, ...,  0.02969041,\n",
       "          0.05296887,  0.07507493]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.0224571 ,  0.04301678, -0.0473253 , ..., -0.02156111,\n",
       "         -0.07501267, -0.04246643],\n",
       "        [-0.01700398, -0.05080323,  0.04795279, ...,  0.06163716,\n",
       "          0.06073893, -0.0662768 ],\n",
       "        [ 0.03070612,  0.03608255, -0.04645684, ...,  0.05802467,\n",
       "          0.06346635,  0.01436663],\n",
       "        ...,\n",
       "        [-0.02923452, -0.05258212, -0.05258764, ...,  0.04442488,\n",
       "          0.06681972,  0.06310006],\n",
       "        [-0.06930365, -0.02154474,  0.01667237, ..., -0.06813588,\n",
       "          0.07379007, -0.05732602],\n",
       "        [ 0.01818274, -0.07134169,  0.04209837, ..., -0.03960317,\n",
       "         -0.02049718, -0.03863262]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[ 0.05681448,  0.05265098, -0.01434405, ..., -0.00458996,\n",
       "         -0.01315719,  0.05278455],\n",
       "        [ 0.01889893,  0.01884126,  0.05635942, ...,  0.02967542,\n",
       "         -0.07158258,  0.02470346],\n",
       "        [ 0.00551267, -0.03596566,  0.05702833, ..., -0.01748017,\n",
       "          0.03795007,  0.0609786 ],\n",
       "        ...,\n",
       "        [-0.02207418,  0.07616051,  0.04485206, ..., -0.00034666,\n",
       "         -0.02253217,  0.02497838],\n",
       "        [ 0.06634935, -0.05473514,  0.0207572 , ...,  0.01373997,\n",
       "         -0.05552974, -0.03025054],\n",
       "        [ 0.03824724,  0.05753743, -0.05770092, ...,  0.03567526,\n",
       "          0.0657628 , -0.03557918]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
       " array([[-0.01901294, -0.0346994 ,  0.02365066, ..., -0.04613863,\n",
       "         -0.02294289, -0.01062015],\n",
       "        [-0.03512465, -0.00956671,  0.04614769, ...,  0.04550041,\n",
       "         -0.00667798,  0.01511838],\n",
       "        [-0.01965017,  0.02442597, -0.03281197, ..., -0.01807056,\n",
       "          0.0364635 ,  0.03541802],\n",
       "        ...,\n",
       "        [ 0.03999863,  0.00041832,  0.00521187, ..., -0.00382004,\n",
       "          0.00868881, -0.03824843],\n",
       "        [-0.00196529,  0.00574039,  0.01884954, ...,  0.04238339,\n",
       "          0.0056036 ,  0.04215534],\n",
       "        [ 0.01907074,  0.03461891,  0.0067586 , ...,  0.04398157,\n",
       "          0.00007848,  0.00831523]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/bias:0' shape=(2048,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
       " array([[-0.04378143, -0.03988087, -0.02305374, ...,  0.03666744,\n",
       "         -0.01386466,  0.01310107],\n",
       "        [-0.03980485, -0.01870906, -0.00156105, ...,  0.03875407,\n",
       "          0.01732334,  0.01932622],\n",
       "        [-0.00135693, -0.00393878, -0.04758455, ..., -0.01281789,\n",
       "         -0.03040434,  0.03956641],\n",
       "        ...,\n",
       "        [ 0.04024035,  0.04811047,  0.00441593, ...,  0.02022742,\n",
       "         -0.03814771, -0.00348662],\n",
       "        [-0.01331812, -0.04101957, -0.02334688, ...,  0.0453084 ,\n",
       "         -0.000524  , -0.02932322],\n",
       "        [ 0.02551839,  0.0361802 ,  0.03912502, ...,  0.04052373,\n",
       "          0.01837836, -0.00899166]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.06375826, -0.06400497,  0.05346561, ..., -0.05691981,\n",
       "         -0.04647036,  0.00539462],\n",
       "        [-0.05929273, -0.04325018,  0.06238494, ..., -0.01568025,\n",
       "         -0.07140822, -0.00790648],\n",
       "        [-0.00829075, -0.0028923 , -0.00834498, ..., -0.02490101,\n",
       "          0.05247945, -0.01541715],\n",
       "        ...,\n",
       "        [-0.00755053, -0.03498329,  0.02771617, ..., -0.05648112,\n",
       "         -0.06326326,  0.00368494],\n",
       "        [ 0.02549014,  0.05162101, -0.05393816, ...,  0.01787171,\n",
       "         -0.02584456, -0.05261625],\n",
       "        [ 0.03859899,  0.0114012 , -0.01101557, ..., -0.02293916,\n",
       "          0.04842018,  0.07185272]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[ 0.0125117 , -0.01340254, -0.02917468, ...,  0.04796708,\n",
       "         -0.05842356,  0.02231302],\n",
       "        [-0.00604951, -0.03350854,  0.04712461, ...,  0.06206279,\n",
       "         -0.06106724,  0.03713901],\n",
       "        [-0.03803898,  0.07468631,  0.071755  , ...,  0.00588468,\n",
       "          0.06110489, -0.02694546],\n",
       "        ...,\n",
       "        [ 0.01886743, -0.02987492,  0.03966364, ..., -0.02063729,\n",
       "          0.01666262, -0.03778541],\n",
       "        [ 0.05934999,  0.02100583,  0.06590081, ...,  0.05117769,\n",
       "          0.04949898, -0.06356725],\n",
       "        [-0.06085218, -0.05688921,  0.06007395, ...,  0.02969041,\n",
       "          0.05296887,  0.07507493]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.0224571 ,  0.04301678, -0.0473253 , ..., -0.02156111,\n",
       "         -0.07501267, -0.04246643],\n",
       "        [-0.01700398, -0.05080323,  0.04795279, ...,  0.06163716,\n",
       "          0.06073893, -0.0662768 ],\n",
       "        [ 0.03070612,  0.03608255, -0.04645684, ...,  0.05802467,\n",
       "          0.06346635,  0.01436663],\n",
       "        ...,\n",
       "        [-0.02923452, -0.05258212, -0.05258764, ...,  0.04442488,\n",
       "          0.06681972,  0.06310006],\n",
       "        [-0.06930365, -0.02154474,  0.01667237, ..., -0.06813588,\n",
       "          0.07379007, -0.05732602],\n",
       "        [ 0.01818274, -0.07134169,  0.04209837, ..., -0.03960317,\n",
       "         -0.02049718, -0.03863262]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[ 0.05681448,  0.05265098, -0.01434405, ..., -0.00458996,\n",
       "         -0.01315719,  0.05278455],\n",
       "        [ 0.01889893,  0.01884126,  0.05635942, ...,  0.02967542,\n",
       "         -0.07158258,  0.02470346],\n",
       "        [ 0.00551267, -0.03596566,  0.05702833, ..., -0.01748017,\n",
       "          0.03795007,  0.0609786 ],\n",
       "        ...,\n",
       "        [-0.02207418,  0.07616051,  0.04485206, ..., -0.00034666,\n",
       "         -0.02253217,  0.02497838],\n",
       "        [ 0.06634935, -0.05473514,  0.0207572 , ...,  0.01373997,\n",
       "         -0.05552974, -0.03025054],\n",
       "        [ 0.03824724,  0.05753743, -0.05770092, ...,  0.03567526,\n",
       "          0.0657628 , -0.03557918]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
       " array([[-0.01901294, -0.0346994 ,  0.02365066, ..., -0.04613863,\n",
       "         -0.02294289, -0.01062015],\n",
       "        [-0.03512465, -0.00956671,  0.04614769, ...,  0.04550041,\n",
       "         -0.00667798,  0.01511838],\n",
       "        [-0.01965017,  0.02442597, -0.03281197, ..., -0.01807056,\n",
       "          0.0364635 ,  0.03541802],\n",
       "        ...,\n",
       "        [ 0.03999863,  0.00041832,  0.00521187, ..., -0.00382004,\n",
       "          0.00868881, -0.03824843],\n",
       "        [-0.00196529,  0.00574039,  0.01884954, ...,  0.04238339,\n",
       "          0.0056036 ,  0.04215534],\n",
       "        [ 0.01907074,  0.03461891,  0.0067586 , ...,  0.04398157,\n",
       "          0.00007848,  0.00831523]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/bias:0' shape=(2048,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
       " array([[-0.04378143, -0.03988087, -0.02305374, ...,  0.03666744,\n",
       "         -0.01386466,  0.01310107],\n",
       "        [-0.03980485, -0.01870906, -0.00156105, ...,  0.03875407,\n",
       "          0.01732334,  0.01932622],\n",
       "        [-0.00135693, -0.00393878, -0.04758455, ..., -0.01281789,\n",
       "         -0.03040434,  0.03956641],\n",
       "        ...,\n",
       "        [ 0.04024035,  0.04811047,  0.00441593, ...,  0.02022742,\n",
       "         -0.03814771, -0.00348662],\n",
       "        [-0.01331812, -0.04101957, -0.02334688, ...,  0.0453084 ,\n",
       "         -0.000524  , -0.02932322],\n",
       "        [ 0.02551839,  0.0361802 ,  0.03912502, ...,  0.04052373,\n",
       "          0.01837836, -0.00899166]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60, 512)\n",
      "--------------------------------------------------\n",
      "(64, 60, 512)\n",
      "(64, 60, 512)\n",
      "--------------------------------------------------\n",
      "(64, 60, 512)\n",
      "(64, 60, 512)\n",
      "(64, 8, 60, 60)\n",
      "(64, 8, 60, 50)\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x -> self attention -> add & normalize & dropout -> out1\n",
    "    out1 , encoding_outputs -> attention -> add & normalize & dropout -> out2\n",
    "    out2 -> ffn -> add & normalize & dropout -> out3\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = feed_forward_network(d_model, dff)\n",
    "        #因为有两个attention，还有一个feed_forward_network，所以有3个\n",
    "        #LayerNormalization和3个dropout\n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        self.layer_norm3 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, encoding_outputs, training,\n",
    "             decoder_mask, encoder_decoder_padding_mask):\n",
    "        # decoder_mask: 由look_ahead_mask和decoder_padding_mask合并而来\n",
    "        \n",
    "        # x.shape: (batch_size, target_seq_len, d_model)\n",
    "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        #按照上面类的注释的步骤依次来编写call实现\n",
    "        # attn1, out1.shape : (batch_size, target_seq_len, d_model)\n",
    "        attn1, attn_weights1 = self.mha1(x, x, x, decoder_mask)\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "        out1 = self.layer_norm1(attn1 + x)\n",
    "        \n",
    "        # attn2, out2.shape : (batch_size, target_seq_len, d_model)\n",
    "        attn2, attn_weights2 = self.mha2(\n",
    "            out1, encoding_outputs, encoding_outputs,\n",
    "            encoder_decoder_padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training = training)\n",
    "        out2 = self.layer_norm2(attn2 + out1)\n",
    "        \n",
    "        # ffn_output, out3.shape: (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layer_norm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights1, attn_weights2\n",
    "\n",
    "#测试一下\n",
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_input = tf.random.uniform((64, 60, 512))\n",
    "sample_decoder_output, sample_decoder_attn_weights1, sample_decoder_attn_weights2 = sample_decoder_layer(\n",
    "    sample_decoder_input, sample_output, False, None, None)\n",
    "\n",
    "print(sample_decoder_output.shape)\n",
    "print(sample_decoder_attn_weights1.shape)  #最后一维60是和x的维度一致的\n",
    "print(sample_decoder_attn_weights2.shape) #最后一维60是和x的维度相关的\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(64, 37, 512)\n",
      "--------------------------------------------------\n",
      "(64, 37, 512)\n",
      "(64, 37, 512)\n",
      "--------------------------------------------------\n",
      "(64, 37, 512)\n",
      "(64, 37, 512)\n"
     ]
    }
   ],
   "source": [
    "#我们多堆建几个EncoderLayer就是我们的EncoderModel\n",
    "class EncoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, input_vocab_size, max_length,\n",
    "                 d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        #这是layers数目\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #构建embedding层\n",
    "        self.embedding = keras.layers.Embedding(input_vocab_size,\n",
    "                                                self.d_model)\n",
    "        # position_embedding.shape: (1, max_length, d_model)\n",
    "        self.position_embedding = get_position_embedding(max_length,\n",
    "                                                         self.d_model)\n",
    "        \n",
    "        self.dropout = keras.layers.Dropout(rate)\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, rate)\n",
    "            for _ in range(self.num_layers)]\n",
    "        \n",
    "    \n",
    "    def call(self, x, training, encoder_padding_mask):\n",
    "        # x.shape: (batch_size, input_seq_len)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal(\n",
    "            input_seq_len, self.max_length,\n",
    "            \"input_seq_len should be less or equal to self.max_length\")\n",
    "        \n",
    "        # x.shape: (batch_size, input_seq_len, d_model)\n",
    "        x = self.embedding(x)\n",
    "        #x做缩放，是值在0到d_model之间\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #因为x长度比position_embedding可能要小，因此embedding切片后和x相加\n",
    "        x += self.position_embedding[:, :input_seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        #得到的x不断作为下一层的输入\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training,\n",
    "                                       encoder_padding_mask)\n",
    "        #x最终shape如下\n",
    "        # x.shape: (batch_size, input_seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "#测试\n",
    "sample_encoder_model = EncoderModel(2, 8500, max_length,\n",
    "                                    512, 8, 2048)\n",
    "sample_encoder_model_input = tf.random.uniform((64, 37))\n",
    "sample_encoder_model_output = sample_encoder_model(\n",
    "    sample_encoder_model_input, False, encoder_padding_mask = None)\n",
    "print(sample_encoder_model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "(64, 8, 35, 35)\n",
      "(64, 8, 35, 37)\n",
      "(64, 8, 35, 35)\n",
      "(64, 8, 35, 37)\n"
     ]
    }
   ],
   "source": [
    "#和encodermodel类似\n",
    "class DecoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, target_vocab_size, max_length,\n",
    "                 d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = keras.layers.Embedding(target_vocab_size,\n",
    "                                                d_model)\n",
    "        self.position_embedding = get_position_embedding(max_length,\n",
    "                                                         d_model)\n",
    "        \n",
    "        self.dropout = keras.layers.Dropout(rate)\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, dff, rate)\n",
    "            for _ in range(self.num_layers)]\n",
    "        \n",
    "    \n",
    "    def call(self, x, encoding_outputs, training,\n",
    "             decoder_mask, encoder_decoder_padding_mask):\n",
    "        # x.shape: (batch_size, output_seq_len)\n",
    "        output_seq_len = tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal(\n",
    "            output_seq_len, self.max_length,\n",
    "            \"output_seq_len should be less or equal to self.max_length\")\n",
    "        \n",
    "        #attention_weights都是由decoder layer返回，把它保存下来\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # x.shape: (batch_size, output_seq_len, d_model)\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.position_embedding[:, :output_seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            #attn1,attn2分别是两个attention\n",
    "            x, attn1, attn2 = self.decoder_layers[i](\n",
    "                x, encoding_outputs, training,\n",
    "                decoder_mask, encoder_decoder_padding_mask)\n",
    "            attention_weights[\n",
    "                'decoder_layer{}_att1'.format(i+1)] = attn1\n",
    "            attention_weights[\n",
    "                'decoder_layer{}_att2'.format(i+1)] = attn2\n",
    "        # x.shape: (batch_size, output_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "\n",
    "sample_decoder_model = DecoderModel(2, 8000, max_length,\n",
    "                                    512, 8, 2048)\n",
    "#测试\n",
    "sample_decoder_model_input = tf.random.uniform((64, 35))\n",
    "sample_decoder_model_output, sample_decoder_model_att \\\n",
    "= sample_decoder_model(\n",
    "    sample_decoder_model_input,\n",
    "    sample_encoder_model_output,#注意这里是encoder的output\n",
    "    training = False, decoder_mask = None,\n",
    "    encoder_decoder_padding_mask = None)\n",
    "\n",
    "print(sample_decoder_model_output.shape)\n",
    "for key in sample_decoder_model_att:\n",
    "    print(sample_decoder_model_att[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(64, 26, 512)\n",
      "--------------------------------------------------\n",
      "(64, 26, 512)\n",
      "(64, 26, 512)\n",
      "--------------------------------------------------\n",
      "(64, 26, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 8000)\n",
      "--------------------------------------------------\n",
      "decoder_layer1_att1 (64, 8, 31, 31)\n",
      "decoder_layer1_att2 (64, 8, 31, 26)\n",
      "decoder_layer2_att1 (64, 8, 31, 31)\n",
      "decoder_layer2_att2 (64, 8, 31, 26)\n"
     ]
    }
   ],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(self, num_layers, input_vocab_size, target_vocab_size,\n",
    "                 max_length, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_model = EncoderModel(\n",
    "            num_layers, input_vocab_size, max_length,\n",
    "            d_model, num_heads, dff, rate)\n",
    "        \n",
    "        self.decoder_model = DecoderModel(\n",
    "            num_layers, target_vocab_size, max_length,\n",
    "            d_model, num_heads, dff, rate)\n",
    "        \n",
    "        self.final_layer = keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, encoder_padding_mask,\n",
    "             decoder_mask, encoder_decoder_padding_mask):\n",
    "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
    "        encoding_outputs = self.encoder_model(\n",
    "            inp, training, encoder_padding_mask)\n",
    "        \n",
    "        # decoding_outputs.shape: (batch_size, output_seq_len, d_model)\n",
    "        decoding_outputs, attention_weights = self.decoder_model(\n",
    "            tar, encoding_outputs, training,\n",
    "            decoder_mask, encoder_decoder_padding_mask)\n",
    "        \n",
    "        # predictions.shape: (batch_size, output_seq_len, target_vocab_size)\n",
    "        predictions = self.final_layer(decoding_outputs)\n",
    "        \n",
    "        return predictions, attention_weights\n",
    "\n",
    "#测试\n",
    "sample_transformer = Transformer(2, 8500, 8000, max_length,\n",
    "                                 512, 8, 2048, rate = 0.1)\n",
    "temp_input = tf.random.uniform((64, 26))\n",
    "temp_target = tf.random.uniform((64, 31))\n",
    "\n",
    "#得到输出\n",
    "predictions, attention_weights = sample_transformer(\n",
    "    temp_input, temp_target, training = False,\n",
    "    encoder_padding_mask = None,\n",
    "    decoder_mask = None,\n",
    "    encoder_decoder_padding_mask = None)\n",
    "#输出shape\n",
    "print(predictions.shape)\n",
    "print('-'*50)\n",
    "#attention_weights 的shape打印\n",
    "for key in attention_weights:\n",
    "    print(key, attention_weights[key].shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. initializes model.\n",
    "# 2. define loss, optimizer, learning_rate schedule\n",
    "# 3. train_step\n",
    "# 4. train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 128)\n",
      "(40, 64)\n",
      "(40, 64)\n",
      "(40, 128)\n",
      "(40, 64)\n",
      "(40, 64)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "#加2是因为最后两个位置是start和end\n",
    "input_vocab_size = pt_tokenizer.vocab_size + 2\n",
    "target_vocab_size = en_tokenizer.vocab_size + 2\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(num_layers,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          max_length,\n",
    "                          d_model, num_heads, dff, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习率变化，是先增后减，因为前期可以快点，后期模型比较好，就要慢点\n",
    "# lrate = (d_model ** -0.5) * min(step_num ** (-0.5),\n",
    "#                                 step_num * warm_up_steps **(-1.5))\n",
    "#自定义的学习率调整设计实现\n",
    "#这里的公式看这里 https://tensorflow.google.cn/tutorials/text/transformer\n",
    "class CustomizedSchedule(\n",
    "    keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps = 4000):\n",
    "        super(CustomizedSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        \n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        \n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomizedSchedule(d_model)\n",
    "optimizer = keras.optimizers.Adam(learning_rate,\n",
    "                                  beta_1 = 0.9,\n",
    "                                  beta_2 = 0.98,\n",
    "                                  epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train step')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomizedSchedule(d_model)\n",
    "#下面是学习率的设计图\n",
    "plt.plot(\n",
    "    temp_learning_rate_schedule(\n",
    "        tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Leraning rate\")\n",
    "plt.xlabel(\"Train step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True, reduction = 'none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    \"\"\"\n",
    "    Encoder:\n",
    "      - encoder_padding_mask (self attention of EncoderLayer)\n",
    "      对于encoder中padding值没作用，所以无需attention\n",
    "    Decoder:\n",
    "      - look_ahead_mask (self attention of DecoderLayer)\n",
    "      target位置上的词不能看到之后的词，因为之后的词没预测出来\n",
    "      - encoder_decoder_padding_mask (encoder-decoder attention of DecoderLayer)\n",
    "      decoder不应该到encoder的padding上去花费精力\n",
    "      - decoder_padding_mask (self attention of DecoderLayer)\n",
    "      decoder也有padding，所以mask掉\n",
    "    \"\"\"\n",
    "    encoder_padding_mask = create_padding_mask(inp)\n",
    "    encoder_decoder_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    decoder_padding_mask = create_padding_mask(tar)\n",
    "    decoder_mask = tf.maximum(decoder_padding_mask,\n",
    "                              look_ahead_mask)\n",
    "    \n",
    "    print( encoder_padding_mask.shape )\n",
    "    print( encoder_decoder_padding_mask.shape )\n",
    "    print( look_ahead_mask.shape )\n",
    "    print( decoder_padding_mask.shape )\n",
    "    print( decoder_mask.shape )\n",
    "\n",
    "    \n",
    "    return encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_inp, temp_tar = iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 37)\n",
      "(64, 38)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 1, 1, 37), dtype=float32, numpy=\n",
       " array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1, 38, 38), dtype=float32, numpy=\n",
       " array([[[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1, 1, 37), dtype=float32, numpy=\n",
       " array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(temp_inp.shape)\n",
    "print(temp_tar.shape)\n",
    "create_masks(temp_inp, temp_tar)\n",
    "#样本大小是64，不足的补齐35，或者39\n",
    "#最后是(64, 1, 39, 39)原因是既不关注前面的padding，也不关注后面的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "Epoch 1 Batch 0 Loss 4.4369 Accuracy 0.0000\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "WARNING:tensorflow:8 out of the last 14 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 14 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "WARNING:tensorflow:9 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "WARNING:tensorflow:6 out of the last 14 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 14 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "WARNING:tensorflow:7 out of the last 15 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 15 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 13 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function train_step at 0x7f2364091158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "Epoch 1 Batch 100 Loss 4.1910 Accuracy 0.0086\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "Epoch 1 Batch 200 Loss 4.0676 Accuracy 0.0129\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "Epoch 1 Batch 300 Loss 3.9159 Accuracy 0.0173\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "Epoch 1 Batch 400 Loss 3.7499 Accuracy 0.0265\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "Epoch 1 Batch 500 Loss 3.6053 Accuracy 0.0337\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "Epoch 1 Batch 600 Loss 3.4837 Accuracy 0.0414\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "Epoch 1 Batch 700 Loss 3.3725 Accuracy 0.0488\n",
      "(31, 1, 1, 36)\n",
      "(31, 1, 1, 36)\n",
      "(33, 33)\n",
      "(31, 1, 1, 33)\n",
      "(31, 1, 33, 33)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "(31, 33, 128)\n",
      "--------------------------------------------------\n",
      "(31, 33, 128)\n",
      "Epoch 1 Loss 3.3696 Accuracy 0.0489\n",
      "Time take for 1 epoch: 927.1389322280884 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.6052 Accuracy 0.0977\n",
      "Epoch 2 Batch 100 Loss 2.6090 Accuracy 0.1061\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "Epoch 2 Batch 200 Loss 2.5310 Accuracy 0.1108\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "Epoch 2 Batch 300 Loss 2.4820 Accuracy 0.1146\n",
      "Epoch 2 Batch 400 Loss 2.4439 Accuracy 0.1174\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 500 Loss 2.4125 Accuracy 0.1203\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "Epoch 2 Batch 600 Loss 2.3877 Accuracy 0.1227\n",
      "Epoch 2 Batch 700 Loss 2.3658 Accuracy 0.1252\n",
      "(31, 1, 1, 37)\n",
      "(31, 1, 1, 37)\n",
      "(32, 32)\n",
      "(31, 1, 1, 32)\n",
      "(31, 1, 32, 32)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "(31, 32, 128)\n",
      "--------------------------------------------------\n",
      "(31, 32, 128)\n",
      "Epoch 2 Loss 2.3665 Accuracy 0.1252\n",
      "Time take for 1 epoch: 516.9084508419037 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.1093 Accuracy 0.1402\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "Epoch 3 Batch 100 Loss 2.1596 Accuracy 0.1434\n",
      "Epoch 3 Batch 200 Loss 2.1481 Accuracy 0.1446\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "Epoch 3 Batch 300 Loss 2.1353 Accuracy 0.1457\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "Epoch 3 Batch 400 Loss 2.1296 Accuracy 0.1467\n",
      "Epoch 3 Batch 500 Loss 2.1214 Accuracy 0.1482\n",
      "Epoch 3 Batch 600 Loss 2.1068 Accuracy 0.1496\n",
      "Epoch 3 Batch 700 Loss 2.0972 Accuracy 0.1515\n",
      "(31, 1, 1, 39)\n",
      "(31, 1, 1, 39)\n",
      "(35, 35)\n",
      "(31, 1, 1, 35)\n",
      "(31, 1, 35, 35)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "Epoch 3 Loss 2.0973 Accuracy 0.1515\n",
      "Time take for 1 epoch: 483.2071418762207 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.9151 Accuracy 0.1503\n",
      "Epoch 4 Batch 100 Loss 1.9472 Accuracy 0.1669\n",
      "Epoch 4 Batch 200 Loss 1.9379 Accuracy 0.1692\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "Epoch 4 Batch 300 Loss 1.9202 Accuracy 0.1710\n",
      "Epoch 4 Batch 400 Loss 1.9067 Accuracy 0.1735\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "Epoch 4 Batch 500 Loss 1.8909 Accuracy 0.1754\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 40, 128)\n",
      "--------------------------------------------------\n",
      "(64, 40, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "Epoch 4 Batch 600 Loss 1.8771 Accuracy 0.1774\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 1, 30)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "Epoch 4 Batch 700 Loss 1.8615 Accuracy 0.1791\n",
      "(31, 1, 1, 40)\n",
      "(31, 1, 1, 40)\n",
      "(39, 39)\n",
      "(31, 1, 1, 39)\n",
      "(31, 1, 39, 39)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "Epoch 4 Loss 1.8613 Accuracy 0.1791\n",
      "Time take for 1 epoch: 507.75655245780945 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.3467 Accuracy 0.1817\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "Epoch 5 Batch 100 Loss 1.7003 Accuracy 0.1967\n",
      "Epoch 5 Batch 200 Loss 1.6945 Accuracy 0.1999\n",
      "Epoch 5 Batch 300 Loss 1.6897 Accuracy 0.2021\n",
      "Epoch 5 Batch 400 Loss 1.6811 Accuracy 0.2035\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "Epoch 5 Batch 500 Loss 1.6685 Accuracy 0.2042\n",
      "Epoch 5 Batch 600 Loss 1.6544 Accuracy 0.2055\n",
      "Epoch 5 Batch 700 Loss 1.6447 Accuracy 0.2064\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 1, 38)\n",
      "(35, 35)\n",
      "(31, 1, 1, 35)\n",
      "(31, 1, 35, 35)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "(31, 35, 128)\n",
      "--------------------------------------------------\n",
      "(31, 35, 128)\n",
      "Epoch 5 Loss 1.6454 Accuracy 0.2065\n",
      "Time take for 1 epoch: 457.131276845932 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.6120 Accuracy 0.2326\n",
      "Epoch 6 Batch 100 Loss 1.4914 Accuracy 0.2239\n",
      "Epoch 6 Batch 200 Loss 1.4910 Accuracy 0.2238\n",
      "Epoch 6 Batch 300 Loss 1.4834 Accuracy 0.2232\n",
      "Epoch 6 Batch 400 Loss 1.4778 Accuracy 0.2237\n",
      "Epoch 6 Batch 500 Loss 1.4715 Accuracy 0.2246\n",
      "Epoch 6 Batch 600 Loss 1.4645 Accuracy 0.2253\n",
      "Epoch 6 Batch 700 Loss 1.4607 Accuracy 0.2263\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 1, 38)\n",
      "(39, 39)\n",
      "(31, 1, 1, 39)\n",
      "(31, 1, 39, 39)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 1.4606 Accuracy 0.2264\n",
      "Time take for 1 epoch: 439.66043972969055 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2945 Accuracy 0.2549\n",
      "Epoch 7 Batch 100 Loss 1.2991 Accuracy 0.2438\n",
      "Epoch 7 Batch 200 Loss 1.3040 Accuracy 0.2445\n",
      "Epoch 7 Batch 300 Loss 1.2992 Accuracy 0.2447\n",
      "Epoch 7 Batch 400 Loss 1.2955 Accuracy 0.2456\n",
      "Epoch 7 Batch 500 Loss 1.2875 Accuracy 0.2459\n",
      "Epoch 7 Batch 600 Loss 1.2814 Accuracy 0.2465\n",
      "Epoch 7 Batch 700 Loss 1.2748 Accuracy 0.2469\n",
      "(31, 1, 1, 37)\n",
      "(31, 1, 1, 37)\n",
      "(38, 38)\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 38, 38)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "Epoch 7 Loss 1.2745 Accuracy 0.2469\n",
      "Time take for 1 epoch: 443.63160395622253 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.2789 Accuracy 0.2825\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "Epoch 8 Batch 100 Loss 1.1353 Accuracy 0.2646\n",
      "Epoch 8 Batch 200 Loss 1.1352 Accuracy 0.2646\n",
      "Epoch 8 Batch 300 Loss 1.1312 Accuracy 0.2640\n",
      "Epoch 8 Batch 400 Loss 1.1282 Accuracy 0.2640\n",
      "Epoch 8 Batch 500 Loss 1.1287 Accuracy 0.2645\n",
      "Epoch 8 Batch 600 Loss 1.1254 Accuracy 0.2648\n",
      "Epoch 8 Batch 700 Loss 1.1242 Accuracy 0.2653\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 1, 38)\n",
      "(38, 38)\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 38, 38)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "Epoch 8 Loss 1.1243 Accuracy 0.2653\n",
      "Time take for 1 epoch: 446.30634450912476 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0078 Accuracy 0.2831\n",
      "Epoch 9 Batch 100 Loss 1.0063 Accuracy 0.2811\n",
      "Epoch 9 Batch 200 Loss 1.0093 Accuracy 0.2808\n",
      "Epoch 9 Batch 300 Loss 1.0112 Accuracy 0.2803\n",
      "Epoch 9 Batch 400 Loss 1.0116 Accuracy 0.2802\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "Epoch 9 Batch 500 Loss 1.0129 Accuracy 0.2796\n",
      "Epoch 9 Batch 600 Loss 1.0139 Accuracy 0.2789\n",
      "Epoch 9 Batch 700 Loss 1.0161 Accuracy 0.2791\n",
      "(31, 1, 1, 40)\n",
      "(31, 1, 1, 40)\n",
      "(38, 38)\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 38, 38)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 40, 128)\n",
      "--------------------------------------------------\n",
      "(31, 40, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "Epoch 9 Loss 1.0164 Accuracy 0.2791\n",
      "Time take for 1 epoch: 449.89526176452637 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.9174 Accuracy 0.2876\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "Epoch 10 Batch 100 Loss 0.9044 Accuracy 0.2907\n",
      "Epoch 10 Batch 200 Loss 0.9128 Accuracy 0.2907\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "Epoch 10 Batch 300 Loss 0.9259 Accuracy 0.2911\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "Epoch 10 Batch 400 Loss 0.9236 Accuracy 0.2901\n",
      "Epoch 10 Batch 500 Loss 0.9262 Accuracy 0.2898\n",
      "Epoch 10 Batch 600 Loss 0.9303 Accuracy 0.2900\n",
      "Epoch 10 Batch 700 Loss 0.9346 Accuracy 0.2898\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 1, 38)\n",
      "(37, 37)\n",
      "(31, 1, 1, 37)\n",
      "(31, 1, 37, 37)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 38, 128)\n",
      "--------------------------------------------------\n",
      "(31, 38, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "Epoch 10 Loss 0.9347 Accuracy 0.2898\n",
      "Time take for 1 epoch: 459.9879071712494 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.8621 Accuracy 0.2977\n",
      "Epoch 11 Batch 100 Loss 0.8443 Accuracy 0.3007\n",
      "Epoch 11 Batch 200 Loss 0.8527 Accuracy 0.3031\n",
      "Epoch 11 Batch 300 Loss 0.8546 Accuracy 0.3014\n",
      "Epoch 11 Batch 400 Loss 0.8574 Accuracy 0.2999\n",
      "Epoch 11 Batch 500 Loss 0.8589 Accuracy 0.2989\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "Epoch 11 Batch 600 Loss 0.8621 Accuracy 0.2986\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 1, 30)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "Epoch 11 Batch 700 Loss 0.8681 Accuracy 0.2982\n",
      "(31, 1, 1, 34)\n",
      "(31, 1, 1, 34)\n",
      "(37, 37)\n",
      "(31, 1, 1, 37)\n",
      "(31, 1, 37, 37)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "Epoch 11 Loss 0.8684 Accuracy 0.2983\n",
      "Time take for 1 epoch: 467.41117668151855 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.7314 Accuracy 0.3174\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 100 Loss 0.7875 Accuracy 0.3112\n",
      "Epoch 12 Batch 200 Loss 0.7893 Accuracy 0.3095\n",
      "Epoch 12 Batch 300 Loss 0.7977 Accuracy 0.3079\n",
      "Epoch 12 Batch 400 Loss 0.8018 Accuracy 0.3075\n",
      "Epoch 12 Batch 500 Loss 0.8063 Accuracy 0.3077\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(28, 28)\n",
      "(64, 1, 1, 28)\n",
      "(64, 1, 28, 28)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "Epoch 12 Batch 600 Loss 0.8079 Accuracy 0.3070\n",
      "Epoch 12 Batch 700 Loss 0.8131 Accuracy 0.3063\n",
      "Epoch 12 Loss 0.8134 Accuracy 0.3063\n",
      "Time take for 1 epoch: 448.34655475616455 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.7327 Accuracy 0.3318\n",
      "Epoch 13 Batch 100 Loss 0.7402 Accuracy 0.3213\n",
      "Epoch 13 Batch 200 Loss 0.7432 Accuracy 0.3185\n",
      "Epoch 13 Batch 300 Loss 0.7441 Accuracy 0.3163\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "Epoch 13 Batch 400 Loss 0.7502 Accuracy 0.3153\n",
      "Epoch 13 Batch 500 Loss 0.7565 Accuracy 0.3141\n",
      "Epoch 13 Batch 600 Loss 0.7628 Accuracy 0.3131\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "(64, 38, 128)\n",
      "--------------------------------------------------\n",
      "(64, 38, 128)\n",
      "Epoch 13 Batch 700 Loss 0.7671 Accuracy 0.3128\n",
      "Epoch 13 Loss 0.7670 Accuracy 0.3127\n",
      "Time take for 1 epoch: 448.7135934829712 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6596 Accuracy 0.3381\n",
      "Epoch 14 Batch 100 Loss 0.6811 Accuracy 0.3233\n",
      "Epoch 14 Batch 200 Loss 0.6978 Accuracy 0.3230\n",
      "Epoch 14 Batch 300 Loss 0.7040 Accuracy 0.3230\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(29, 29)\n",
      "(64, 1, 1, 29)\n",
      "(64, 1, 29, 29)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "Epoch 14 Batch 400 Loss 0.7112 Accuracy 0.3226\n",
      "Epoch 14 Batch 500 Loss 0.7175 Accuracy 0.3216\n",
      "Epoch 14 Batch 600 Loss 0.7228 Accuracy 0.3205\n",
      "Epoch 14 Batch 700 Loss 0.7277 Accuracy 0.3193\n",
      "(31, 1, 1, 37)\n",
      "(31, 1, 1, 37)\n",
      "(36, 36)\n",
      "(31, 1, 1, 36)\n",
      "(31, 1, 36, 36)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 37, 128)\n",
      "--------------------------------------------------\n",
      "(31, 37, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "Epoch 14 Loss 0.7280 Accuracy 0.3194\n",
      "Time take for 1 epoch: 456.71864080429077 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.6661 Accuracy 0.3125\n",
      "Epoch 15 Batch 100 Loss 0.6587 Accuracy 0.3314\n",
      "Epoch 15 Batch 200 Loss 0.6617 Accuracy 0.3272\n",
      "Epoch 15 Batch 300 Loss 0.6680 Accuracy 0.3266\n",
      "Epoch 15 Batch 400 Loss 0.6731 Accuracy 0.3262\n",
      "Epoch 15 Batch 500 Loss 0.6767 Accuracy 0.3247\n",
      "Epoch 15 Batch 600 Loss 0.6825 Accuracy 0.3241\n",
      "Epoch 15 Batch 700 Loss 0.6881 Accuracy 0.3233\n",
      "(31, 1, 1, 39)\n",
      "(31, 1, 1, 39)\n",
      "(36, 36)\n",
      "(31, 1, 1, 36)\n",
      "(31, 1, 36, 36)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "Epoch 15 Loss 0.6885 Accuracy 0.3233\n",
      "Time take for 1 epoch: 444.9080696105957 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.6709 Accuracy 0.3181\n",
      "Epoch 16 Batch 100 Loss 0.6123 Accuracy 0.3341\n",
      "Epoch 16 Batch 200 Loss 0.6272 Accuracy 0.3331\n",
      "Epoch 16 Batch 300 Loss 0.6358 Accuracy 0.3320\n",
      "Epoch 16 Batch 400 Loss 0.6409 Accuracy 0.3306\n",
      "Epoch 16 Batch 500 Loss 0.6465 Accuracy 0.3292\n",
      "Epoch 16 Batch 600 Loss 0.6517 Accuracy 0.3285\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(29, 29)\n",
      "(64, 1, 1, 29)\n",
      "(64, 1, 29, 29)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 36, 128)\n",
      "--------------------------------------------------\n",
      "(64, 36, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 39, 128)\n",
      "--------------------------------------------------\n",
      "(64, 39, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "Epoch 16 Batch 700 Loss 0.6570 Accuracy 0.3280\n",
      "(31, 1, 1, 36)\n",
      "(31, 1, 1, 36)\n",
      "(39, 39)\n",
      "(31, 1, 1, 39)\n",
      "(31, 1, 39, 39)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 36, 128)\n",
      "--------------------------------------------------\n",
      "(31, 36, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "(31, 39, 128)\n",
      "--------------------------------------------------\n",
      "(31, 39, 128)\n",
      "Epoch 16 Loss 0.6570 Accuracy 0.3280\n",
      "Time take for 1 epoch: 461.42556500434875 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.6218 Accuracy 0.3381\n",
      "Epoch 17 Batch 100 Loss 0.5830 Accuracy 0.3391\n",
      "Epoch 17 Batch 200 Loss 0.5966 Accuracy 0.3377\n",
      "Epoch 17 Batch 300 Loss 0.6041 Accuracy 0.3365\n",
      "Epoch 17 Batch 400 Loss 0.6112 Accuracy 0.3352\n",
      "Epoch 17 Batch 500 Loss 0.6173 Accuracy 0.3343\n",
      "Epoch 17 Batch 600 Loss 0.6242 Accuracy 0.3333\n",
      "Epoch 17 Batch 700 Loss 0.6286 Accuracy 0.3324\n",
      "Epoch 17 Loss 0.6287 Accuracy 0.3324\n",
      "Time take for 1 epoch: 429.94839453697205 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.6175 Accuracy 0.3853\n",
      "Epoch 18 Batch 100 Loss 0.5690 Accuracy 0.3437\n",
      "Epoch 18 Batch 200 Loss 0.5751 Accuracy 0.3414\n",
      "Epoch 18 Batch 300 Loss 0.5795 Accuracy 0.3407\n",
      "Epoch 18 Batch 400 Loss 0.5869 Accuracy 0.3385\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 1, 30)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "(64, 33, 128)\n",
      "--------------------------------------------------\n",
      "(64, 33, 128)\n",
      "Epoch 18 Batch 500 Loss 0.5950 Accuracy 0.3382\n",
      "Epoch 18 Batch 600 Loss 0.6010 Accuracy 0.3376\n",
      "(64, 1, 1, 28)\n",
      "(64, 1, 1, 28)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 28, 128)\n",
      "--------------------------------------------------\n",
      "(64, 28, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "Epoch 18 Batch 700 Loss 0.6045 Accuracy 0.3365\n",
      "Epoch 18 Loss 0.6047 Accuracy 0.3366\n",
      "Time take for 1 epoch: 443.2289798259735 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5111 Accuracy 0.3442\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 37, 128)\n",
      "--------------------------------------------------\n",
      "(64, 37, 128)\n",
      "(64, 1, 1, 29)\n",
      "(64, 1, 1, 29)\n",
      "(29, 29)\n",
      "(64, 1, 1, 29)\n",
      "(64, 1, 29, 29)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "(64, 29, 128)\n",
      "--------------------------------------------------\n",
      "(64, 29, 128)\n",
      "Epoch 19 Batch 100 Loss 0.5490 Accuracy 0.3471\n",
      "Epoch 19 Batch 200 Loss 0.5533 Accuracy 0.3452\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(27, 27)\n",
      "(64, 1, 1, 27)\n",
      "(64, 1, 27, 27)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 32, 128)\n",
      "--------------------------------------------------\n",
      "(64, 32, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "(64, 27, 128)\n",
      "--------------------------------------------------\n",
      "(64, 27, 128)\n",
      "Epoch 19 Batch 300 Loss 0.5620 Accuracy 0.3441\n",
      "Epoch 19 Batch 400 Loss 0.5651 Accuracy 0.3416\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 31, 128)\n",
      "--------------------------------------------------\n",
      "(64, 31, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "(64, 35, 128)\n",
      "--------------------------------------------------\n",
      "(64, 35, 128)\n",
      "Epoch 19 Batch 500 Loss 0.5708 Accuracy 0.3417\n",
      "Epoch 19 Batch 600 Loss 0.5771 Accuracy 0.3408\n",
      "Epoch 19 Batch 700 Loss 0.5819 Accuracy 0.3402\n",
      "(31, 1, 1, 34)\n",
      "(31, 1, 1, 34)\n",
      "(34, 34)\n",
      "(31, 1, 1, 34)\n",
      "(31, 1, 34, 34)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "(31, 34, 128)\n",
      "--------------------------------------------------\n",
      "(31, 34, 128)\n",
      "Epoch 19 Loss 0.5821 Accuracy 0.3402\n",
      "Time take for 1 epoch: 470.37521839141846 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.5842 Accuracy 0.3562\n",
      "Epoch 20 Batch 100 Loss 0.5265 Accuracy 0.3534\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 1, 30)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 30, 128)\n",
      "--------------------------------------------------\n",
      "(64, 30, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "(64, 34, 128)\n",
      "--------------------------------------------------\n",
      "(64, 34, 128)\n",
      "Epoch 20 Batch 200 Loss 0.5292 Accuracy 0.3491\n",
      "Epoch 20 Batch 300 Loss 0.5380 Accuracy 0.3462\n",
      "Epoch 20 Batch 400 Loss 0.5446 Accuracy 0.3456\n"
     ]
    }
   ],
   "source": [
    "train_loss = keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "    name = 'train_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp  = tar[:, :-1]  #没带end\n",
    "    tar_real = tar[:, 1:]   #没有start\n",
    "    \n",
    "    encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "    = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True,\n",
    "                                     encoder_padding_mask,\n",
    "                                     decoder_mask,\n",
    "                                     encoder_decoder_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "#一个epochs接近90秒\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    #reset后就会从零开始累计\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result(),\n",
    "                train_accuracy.result()))\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "        epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "    print('Time take for 1 epoch: {} secs\\n'.format(\n",
    "        time.time() - start))\n",
    "\n",
    "#loss是一个正常的指标，accuracy只是机器翻译的一个参考指标，可以看趋势\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eg: A B C D -> E F G H.\n",
    "Train: A B C D, E F G -> F G H\n",
    "Eval:  A B C D -> E\n",
    "       A B C D, E -> F\n",
    "       A B C D, E F -> G\n",
    "       A B C D, E F G -> H\n",
    "类似seq2seq2\n",
    "不同的是 transformer可以并行的处理，前后没有依赖，而seq2seq前后有依赖\n",
    "\"\"\"\n",
    "def evaluate(inp_sentence):\n",
    "    #文本的句子转换为id的句子\n",
    "    input_id_sentence = [pt_tokenizer.vocab_size] \\\n",
    "    + pt_tokenizer.encode(inp_sentence) + [pt_tokenizer.vocab_size + 1]\n",
    "    #transformer转换是两维的，因此转换\n",
    "    # encoder_input.shape: (1, input_sentence_length)\n",
    "    encoder_input = tf.expand_dims(input_id_sentence, 0)\n",
    "    \n",
    "    # decoder_input.shape: (1, 1)\n",
    "    #我们预测一个词就放入decoder_input，decoder_input给多个就可以预测多个，我们给一个\n",
    "    decoder_input = tf.expand_dims([en_tokenizer.vocab_size], 0)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        #产生mask并传给transformer\n",
    "        encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "        = create_masks(encoder_input, decoder_input)\n",
    "        # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            decoder_input,\n",
    "            False,\n",
    "            encoder_padding_mask,\n",
    "            decoder_mask,\n",
    "            encoder_decoder_padding_mask)\n",
    "        # predictions.shape: (batch_size, target_vocab_size)\n",
    "        #我们每次只预测一个，所以是最后一个\n",
    "        predictions = predictions[:, -1, :]\n",
    "        #预测值就是概率最大的那个的索引\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis = -1),\n",
    "                               tf.int32)\n",
    "        #如果等于end id，预测结束\n",
    "        if tf.equal(predicted_id, en_tokenizer.vocab_size + 1):\n",
    "            return tf.squeeze(decoder_input, axis = 0), attention_weights\n",
    "        #如果predicted_id不是end id，添加到新的decoder_input中\n",
    "        decoder_input = tf.concat([decoder_input, [predicted_id]],\n",
    "                                  axis = -1)\n",
    "    return tf.squeeze(decoder_input, axis = 0), attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_decoder_attention(attention, input_sentence,\n",
    "                                   result, layer_name):\n",
    "    fig = plt.figure(figsize = (16, 8))\n",
    "    \n",
    "    input_id_sentence = pt_tokenizer.encode(input_sentence)\n",
    "    \n",
    "    # attention.shape: (num_heads, tar_len, input_len)\n",
    "    attention = tf.squeeze(attention[layer_name], axis = 0)\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)\n",
    "        \n",
    "        ax.matshow(attention[head][:-1, :])\n",
    "        \n",
    "        fontdict = {'fontsize': 10}\n",
    "        \n",
    "        ax.set_xticks(range(len(input_id_sentence) + 2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "        \n",
    "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
    "        \n",
    "        ax.set_xticklabels(\n",
    "            ['<start>'] + [pt_tokenizer.decode([i]) for i in input_id_sentence] + ['<end>'],\n",
    "            fontdict = fontdict, rotation = 90)\n",
    "        ax.set_yticklabels(\n",
    "            [en_tokenizer.decode([i]) for i in result if i < en_tokenizer.vocab_size],\n",
    "            fontdict = fontdict)\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, layer_name = ''):\n",
    "    result, attention_weights = evaluate(input_sentence)\n",
    "    \n",
    "    predicted_sentence = en_tokenizer.decode(\n",
    "        [i for i in result if i < en_tokenizer.vocab_size])\n",
    "    \n",
    "    print(\"Input: {}\".format(input_sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))\n",
    "    \n",
    "    if layer_name:\n",
    "        plot_encoder_decoder_attention(attention_weights, input_sentence,\n",
    "                                       result, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('está muito frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('isto é minha vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('você ainda está em casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('este é o primeiro livro que eu já li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('este é o primeiro livro que eu já li',\n",
    "          layer_name = 'decoder_layer4_att2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
